#!/usr/bin/env python3
"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FBREF JSON TO DATABASE V2.0 - PERFECTION 150/150 M√âTRIQUES
Extrait 2299 joueurs √ó 150 m√©triques du JSON vers PostgreSQL
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Version: 2.0 - Dynamic Parsing (Hedge Fund Grade)
Cr√©√©: 2025-12-18
Auteur: Mon_PS Team
Source: /home/Mon_ps/data/fbref/fbref_players_clean_2025_26.json
Target: Table fbref_player_stats_full (163 colonnes) + player_stats (legacy)
Mapping: /tmp/fbref_column_mapping.json (150 m√©triques JSON ‚Üí SQL)
"""

import json
import psycopg2
import psycopg2.extras
import logging
import unicodedata
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

# Configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'monps_db',
    'user': 'monps_user',
    'password': 'monps_secure_password_2024'
}

JSON_PATH = '/home/Mon_ps/data/fbref/fbref_players_clean_2025_26.json'
COLUMN_MAPPING_PATH = '/tmp/fbref_column_mapping.json'
SEASON = '2025-2026'


def normalize_name(name: str) -> str:
    """Normalise le nom pour matching"""
    name = unicodedata.normalize('NFD', name)
    name = ''.join(c for c in name if unicodedata.category(c) != 'Mn')
    return name.lower().strip()


def safe_numeric(value: Any) -> Optional[float]:
    """Conversion s√©curis√©e vers numeric"""
    if value is None or value == '' or value == '-':
        return None
    try:
        return float(str(value).replace(',', '').replace('%', ''))
    except (ValueError, TypeError):
        return None


def load_column_mapping() -> Dict[str, str]:
    """Charge le mapping JSON ‚Üí SQL depuis le fichier de configuration"""
    try:
        with open(COLUMN_MAPPING_PATH, 'r') as f:
            mapping = json.load(f)
        logger.info(f"   ‚úÖ Mapping charg√©: {len(mapping)} colonnes")
        return mapping
    except FileNotFoundError:
        logger.error(f"   ‚ùå Mapping non trouv√©: {COLUMN_MAPPING_PATH}")
        return {}


def parse_player_dynamic(player_name: str, player_data: Dict, column_mapping: Dict) -> Dict:
    """
    Parse un joueur depuis le JSON vers un dict pr√™t pour insertion DB.
    Utilise le mapping dynamique pour extraire TOUTES les m√©triques.
    """
    stats = player_data.get('stats', {})

    # Base fields
    record = {
        'player_name': player_name,
        'player_name_normalized': normalize_name(player_name),
        'team': player_data.get('team', ''),
        'league': player_data.get('league', ''),
        'season': SEASON,
        'position': player_data.get('position', ''),
        'age': int(player_data.get('age', 0)) if player_data.get('age') else None,
        'nationality': player_data.get('nation', ''),
        'source': 'fbref',
        'scraped_at': player_data.get('scraped_at'),
    }

    # Dynamically map all 150 metrics from stats dict
    for json_key, sql_column in column_mapping.items():
        # Try exact match first
        value = stats.get(json_key)

        # If not found, try case-insensitive search for mixed case keys
        if value is None:
            # Try common variations (xG, npxG, xA, etc.)
            for key in stats.keys():
                if key.lower() == json_key.lower():
                    value = stats.get(key)
                    break

        record[sql_column] = safe_numeric(value)

    return record


def get_dynamic_columns(conn) -> List[str]:
    """R√©cup√®re la liste des colonnes disponibles dans fbref_player_stats_full"""
    cur = conn.cursor()
    cur.execute("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'fbref_player_stats_full'
        ORDER BY ordinal_position
    """)
    columns = [row[0] for row in cur.fetchall()]
    cur.close()

    # Exclure les colonnes auto-g√©n√©r√©es
    exclude = ['id', 'inserted_at', 'updated_at']
    columns = [c for c in columns if c not in exclude]

    return columns


def insert_players_dynamic(records: list) -> int:
    """Insert/Update players dans fbref_player_stats_full avec toutes les m√©triques"""
    if not records:
        return 0

    conn = psycopg2.connect(**DB_CONFIG)

    # R√©cup√©rer les colonnes disponibles dans la table
    available_columns = get_dynamic_columns(conn)
    logger.info(f"   üìä Colonnes disponibles dans la table: {len(available_columns)}")

    # Filtrer les colonnes pr√©sentes dans les records
    sample_record = records[0]
    columns_to_insert = [col for col in available_columns if col in sample_record]
    logger.info(f"   üìä Colonnes √† ins√©rer: {len(columns_to_insert)}")

    # Construire dynamiquement la requ√™te INSERT
    column_names = ', '.join(columns_to_insert)
    placeholders = ', '.join([f'%({col})s' for col in columns_to_insert])

    # Construire les clauses UPDATE
    update_clauses = ', '.join([
        f"{col} = EXCLUDED.{col}"
        for col in columns_to_insert
        if col not in ['player_name', 'team', 'league', 'season']
    ])

    insert_sql = f"""
        INSERT INTO fbref_player_stats_full ({column_names}, updated_at)
        VALUES ({placeholders}, NOW())
        ON CONFLICT (player_name, team, league, season)
        DO UPDATE SET
            {update_clauses},
            updated_at = NOW()
    """

    cur = conn.cursor()
    inserted = 0
    errors = 0

    for record in records:
        try:
            cur.execute(insert_sql, record)
            inserted += 1
        except Exception as e:
            errors += 1
            if errors <= 5:
                logger.warning(f"Erreur insert {record.get('player_name')}: {e}")
            conn.rollback()
            continue

    conn.commit()
    cur.close()
    conn.close()

    return inserted


def audit_completeness():
    """
    Audit Hedge Fund: V√©rifie que toutes les 150 m√©triques sont bien remplies.
    Retourne statistiques de compl√©tude par colonne.
    """
    logger.info("\n" + "=" * 70)
    logger.info("üìä AUDIT HEDGE FUND - COMPL√âTUDE DES 150 M√âTRIQUES")
    logger.info("=" * 70)

    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    # R√©cup√©rer toutes les colonnes m√©triques
    cur.execute("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'fbref_player_stats_full'
        AND column_name NOT IN (
            'id', 'player_name', 'player_name_normalized', 'team', 'league',
            'season', 'position', 'age', 'nationality', 'source',
            'scraped_at', 'inserted_at', 'updated_at'
        )
        ORDER BY column_name
    """)
    metric_columns = [row[0] for row in cur.fetchall()]

    logger.info(f"\nüìà Colonnes m√©triques trouv√©es: {len(metric_columns)}")

    # Compter les valeurs NULL par colonne
    cur.execute("SELECT COUNT(*) FROM fbref_player_stats_full")
    total_players = cur.fetchone()[0]

    completeness_report = []
    empty_columns = []
    perfect_columns = []

    for column in metric_columns:
        cur.execute(f"""
            SELECT
                COUNT(*) as total,
                COUNT({column}) as non_null,
                COUNT(*) - COUNT({column}) as null_count
            FROM fbref_player_stats_full
        """)
        total, non_null, null_count = cur.fetchone()
        completeness_pct = (non_null / total * 100) if total > 0 else 0

        completeness_report.append({
            'column': column,
            'non_null': non_null,
            'null_count': null_count,
            'completeness_pct': completeness_pct
        })

        if completeness_pct == 0:
            empty_columns.append(column)
        elif completeness_pct == 100:
            perfect_columns.append(column)

    # Statistiques globales
    avg_completeness = sum(r['completeness_pct'] for r in completeness_report) / len(completeness_report)

    logger.info(f"\nüìä STATISTIQUES GLOBALES:")
    logger.info(f"   Total joueurs: {total_players}")
    logger.info(f"   Total m√©triques: {len(metric_columns)}")
    logger.info(f"   Compl√©tude moyenne: {avg_completeness:.1f}%")
    logger.info(f"   Colonnes parfaites (100%): {len(perfect_columns)}")
    logger.info(f"   Colonnes vides (0%): {len(empty_columns)}")

    # Top 10 colonnes les mieux remplies
    logger.info(f"\n‚úÖ TOP 10 COLONNES LES MIEUX REMPLIES:")
    top_filled = sorted(completeness_report, key=lambda x: -x['completeness_pct'])[:10]
    for i, col_data in enumerate(top_filled, 1):
        logger.info(f"   {i:2d}. {col_data['column']:30s} ‚Üí {col_data['completeness_pct']:5.1f}% ({col_data['non_null']:4d}/{total_players})")

    # Colonnes vides (si pr√©sentes)
    if empty_columns:
        logger.info(f"\n‚ö†Ô∏è  COLONNES VIDES ({len(empty_columns)}):")
        for col in empty_columns[:20]:  # Limiter √† 20 pour l'affichage
            logger.info(f"   ‚îî‚îÄ {col}")

    cur.close()
    conn.close()

    logger.info("=" * 70)

    return {
        'total_metrics': len(metric_columns),
        'avg_completeness': avg_completeness,
        'perfect_columns': len(perfect_columns),
        'empty_columns': len(empty_columns)
    }


def update_legacy_player_stats():
    """
    Met √† jour la table player_stats legacy pour compatibilit√©.
    Avec gestion robuste des contraintes manquantes.
    """
    logger.info("\nüìä Mise √† jour table legacy player_stats...")

    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    try:
        # La contrainte UNIQUE existante est (player_name, team_name, season)
        # On l'utilise pour l'UPSERT
        cur.execute("""
            INSERT INTO player_stats (
                player_name, team_name, league, season,
                goals, assists, minutes, xg, npxg, xa,
                shots, shots_on_target, position,
                sca, gca,
                source, updated_at
            )
            SELECT
                player_name, team, league, season,
                COALESCE(goals, 0)::int,
                COALESCE(assists, 0)::int,
                COALESCE(minutes, 0)::int,
                xg, npxg, xa,
                COALESCE(shots, 0)::int,
                COALESCE(shots_on_target, 0)::int,
                position,
                COALESCE(shot_creating_actions, 0)::int,
                COALESCE(goal_creating_actions, 0)::int,
                'fbref', NOW()
            FROM fbref_player_stats_full
            WHERE season = %s
            ON CONFLICT (player_name, team_name, season)
            DO UPDATE SET
                league = EXCLUDED.league,
                goals = EXCLUDED.goals,
                assists = EXCLUDED.assists,
                minutes = EXCLUDED.minutes,
                xg = EXCLUDED.xg,
                npxg = EXCLUDED.npxg,
                xa = EXCLUDED.xa,
                shots = EXCLUDED.shots,
                shots_on_target = EXCLUDED.shots_on_target,
                position = EXCLUDED.position,
                sca = EXCLUDED.sca,
                gca = EXCLUDED.gca,
                updated_at = NOW()
        """, (SEASON,))
        updated = cur.rowcount
        conn.commit()
        logger.info(f"   ‚úÖ {updated} joueurs mis √† jour dans player_stats")
    except Exception as e:
        logger.error(f"   ‚ùå Erreur mise √† jour legacy: {e}")
        conn.rollback()
    finally:
        cur.close()
        conn.close()


def main():
    """Point d'entr√©e principal"""
    logger.info("=" * 70)
    logger.info("FBREF JSON TO DATABASE V2.0 - PERFECTION 150/150")
    logger.info(f"Source: {JSON_PATH}")
    logger.info(f"Saison: {SEASON}")
    logger.info("=" * 70)

    # Charger mapping colonnes
    logger.info("\nüìÇ Chargement mapping colonnes...")
    column_mapping = load_column_mapping()
    if not column_mapping:
        logger.error("‚ùå Impossible de continuer sans mapping")
        return

    # Charger JSON
    logger.info("\nüìÇ Chargement JSON FBRef...")
    try:
        with open(JSON_PATH, 'r') as f:
            data = json.load(f)
    except FileNotFoundError:
        logger.error(f"‚ùå Fichier non trouv√©: {JSON_PATH}")
        return

    players = data.get('players', {})
    metadata = data.get('metadata', {})

    logger.info(f"   ‚úÖ {len(players)} joueurs trouv√©s")
    logger.info(f"   üìÖ Scraped: {metadata.get('scraped_date', 'N/A')}")

    # Parser tous les joueurs avec mapping dynamique
    logger.info("\nüîÑ Parsing joueurs (150 m√©triques dynamiques)...")
    records = []
    for player_name, player_data in players.items():
        record = parse_player_dynamic(player_name, player_data, column_mapping)
        records.append(record)

    logger.info(f"   ‚úÖ {len(records)} joueurs pars√©s")

    # Stats par ligue
    by_league = {}
    for r in records:
        league = r.get('league', 'Unknown')
        by_league[league] = by_league.get(league, 0) + 1

    logger.info("\nüìä Distribution par ligue:")
    for league, count in sorted(by_league.items(), key=lambda x: -x[1]):
        logger.info(f"   ‚îî‚îÄ {league}: {count} joueurs")

    # Ins√©rer en DB avec toutes les m√©triques
    logger.info("\nüíæ Insertion dans fbref_player_stats_full (150 m√©triques)...")
    inserted = insert_players_dynamic(records)
    logger.info(f"   ‚úÖ {inserted}/{len(records)} joueurs ins√©r√©s/mis √† jour ({inserted * 100 / len(records):.1f}%)")

    # Audit de compl√©tude
    audit_results = audit_completeness()

    # Mettre √† jour table legacy
    update_legacy_player_stats()

    # R√©sum√© final
    logger.info(f"\n{'=' * 70}")
    logger.info("üèÅ TERMIN√â - VERSION 2.0 PERFECTION")
    logger.info(f"   Joueurs trait√©s: {len(records)}")
    logger.info(f"   Ins√©r√©s/Mis √† jour: {inserted}")
    logger.info(f"   Taux succ√®s: {inserted * 100 / len(records):.1f}%")
    logger.info(f"   M√©triques totales: {audit_results['total_metrics']}")
    logger.info(f"   Compl√©tude moyenne: {audit_results['avg_completeness']:.1f}%")
    logger.info(f"   Colonnes parfaites: {audit_results['perfect_columns']}")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()
